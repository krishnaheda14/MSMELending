# MSME Lending Solution â€” Indian Financial Data Lake

## ğŸ¯ Problem Statement

**Challenge**: Traditional MSME lending in India faces critical barriers:
- **Manual, paper-heavy credit assessment** leads to weeks-long approval cycles
- **Single-source bureau scores** (CIBIL) miss 70% of the real financial picture
- **No unified view** of banking, GST, insurance, mutual funds, ONDC, OCEN data
- **DPDP Act 2023** mandates explicit per-customer consent â€” bulk data operations now prohibited
- **RBI Account Aggregator Framework** requires standardized multi-FIP data fetching


  - **Live logs with timestamps** â€” color-coded by severity (error/warning/success/info)
- **Dataset Viewer** â€” raw/clean data inspection with limits

### AI Integration
- **Deepseek API** (primary) â€” OpenAI-compatible endpoint
- **Google Gemini** (fallback) â€” robust parsing for varied response shapes
- Automatic fallback if Deepseek fails
- Token limits enforced (prompt + response configurable via env)

### Compliance & Security
- **DPDP Act 2023**: All operations require explicit `customer_id` â€” no bulk queries
- **RBI AA Framework**: Simulated consent flow and multi-FIP data aggregation
- `.gitignore` excludes raw data files (`data_lake/raw/*.ndjson`)

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- Node.js 18+
- Git

### 1. Clone & Setup Backend
```bash
cd F:\MSMELending\data_lake

# Install Python dependencies
pip install -r requirements.txt

# Create .env file with API keys
echo "DEEPSEEK_API_KEY=sk-your deepseek key here" > .env
echo "GEMINI_API_KEY=your_gemini_key_here" >> .env
```

### 2. Generate Data for a Customer
```bash
# Generate raw data for CUST_MSM_00001
python generate_all.py --customer-id CUST_MSM_00001

# Clean the data
python pipeline/clean_data.py --customer-id CUST_MSM_00001

# Generate analytics
python analytics/generate_summaries.py --customer-id CUST_MSM_00001
```

### 3. Start Backend API
```bash
cd api_panel
python app.py
```
Backend runs at `http://localhost:5000`

### 4. Start Frontend
```bash
cd ../frontend
npm install
npm start
```
Frontend runs at `http://localhost:3000`

### 5. Open Dashboard
- Navigate to `http://localhost:3000`
- Enter customer ID (e.g., `CUST_MSM_00001`) and press **Enter** or click "Load Analytics"
- Click "Get AI Insights" for lending recommendation
- Explore Methodology and Calculations tabs

---

## ğŸ“– Usage Guide

### Generate Data for Multiple Customers

**Option 1: Via Pipeline Monitor UI (Recommended)**
1. Open `http://localhost:3000` â†’ go to **Pipeline Monitor** tab
2. Click **"Generate Random Customer ID"** button (green section at top)
3. A new random customer ID will be assigned (e.g., `CUST_MSM_47832`)
4. Click pipeline steps in order to generate data for that customer:
   - Step 1: Validate Consent & Fetch Data
   - Step 2: Clean & Validate Data
   - Step 3: Generate Analytics & Insights
   - Step 4: Calculate Credit Score
5. **Debug panel** shows real-time execution status and current command being run
6. Repeat to add more customers to your data pool

**Option 2: Via Command Line**
```bash
# Customer 1
python generate_all.py --customer-id CUST_MSM_00001
python pipeline/clean_data.py --customer-id CUST_MSM_00001
python analytics/generate_summaries.py --customer-id CUST_MSM_00001

# Customer 2 (will have different random seed â†’ unique data)
python generate_all.py --customer-id CUST_MSM_00002
python pipeline/clean_data.py --customer-id CUST_MSM_00002
python analytics/generate_summaries.py --customer-id CUST_MSM_00002
```

**Note**: Each `customer_id` is hashed to seed the random number generator, ensuring reproducible yet distinct data per customer.

### Run Full Pipeline via UI
1. Open `http://localhost:3000`
2. Go to **Pipeline Monitor** tab
3. Enter customer ID (required)
4. Click pipeline steps in order:
   - Step 1: Generate Data
   - Step 2: Clean Data
   - Step 3: Generate Analytics
   - Step 4: Calculate Credit Score

### Debug Data Issues
- Use **Show Debug** button in GST & Business Insights section to inspect raw data structure
- Check `logs/` directory for validation errors and cleaning logs
- Inspect `raw/` vs `clean/` NDJSON files in Dataset Viewer tab

---

## ğŸ§ª Testing & Verification

### Test Credit Score Calculation
```bash
curl -X POST http://localhost:5000/api/pipeline/calculate_score \
  -H "Content-Type: application/json" \
  -d '{"customer_id":"CUST_MSM_00001"}'
```

### Test Analytics Endpoint
```bash
curl "http://localhost:5000/api/analytics?customer_id=CUST_MSM_00001"
```

### Test AI Insights
```bash
curl -X POST http://localhost:5000/api/ai-insights \
  -H "Content-Type: application/json" \
  -d '{"customer_id":"CUST_MSM_00001"}'
```

---

## ğŸ“‚ Project Structure

```
MSMELending/
â”œâ”€â”€ data_lake/
â”‚   â”œâ”€â”€ generators/              # Synthetic data generators
â”‚   â”‚   â”œâ”€â”€ generate_banking_data.py
â”‚   â”‚   â”œâ”€â”€ generate_additional_data.py  (GST, Credit Bureau)
â”‚   â”‚   â”œâ”€â”€ generate_insurance_mf.py
â”‚   â”‚   â”œâ”€â”€ generate_ondc_ocen.py
â”‚   â”‚   â””â”€â”€ indian_data_utils.py
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â””â”€â”€ clean_data.py        # Data cleaning & validation
â”‚   â”œâ”€â”€ analytics/
â”‚   â”‚   â””â”€â”€ generate_summaries.py  # Analytics engine
â”‚   â”œâ”€â”€ api_panel/
â”‚   â”‚   â””â”€â”€ app.py               # Flask API + SocketIO
â”‚   â”œâ”€â”€ frontend/
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ components/
â”‚   â”‚       â”‚   â”œâ”€â”€ AnalyticsInsights.js  (main dashboard)
â”‚   â”‚       â”‚   â”œâ”€â”€ CreditMethodology.js  (explainability doc)
â”‚   â”‚       â”‚   â”œâ”€â”€ CreditCalculations.js (numeric examples)
â”‚   â”‚       â”‚   â”œâ”€â”€ PipelineMonitor.js
â”‚   â”‚       â”‚   â”œâ”€â”€ DatasetViewer.js
â”‚   â”‚       â”‚   â””â”€â”€ Sidebar.js
â”‚   â”‚       â””â”€â”€ App.js
â”‚   â”œâ”€â”€ schemas/                 # JSON schemas for validation
â”‚   â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ raw/                     # Raw NDJSON data (gitignored)
â”‚   â”œâ”€â”€ clean/                   # Cleaned NDJSON data (gitignored)
â”‚   â”œâ”€â”€ analytics/               # Analytics JSON summaries (gitignored)
â”‚   â”œâ”€â”€ logs/                    # Validation error logs (gitignored)
â”‚   â”œâ”€â”€ config.json              # Generation config
â”‚   â”œâ”€â”€ generate_all.py          # Master generator orchestrator
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â””â”€â”€ README.md                    # This file
```

---

## ğŸ”§ Configuration

### Scale Settings (`config.json`)
```json
{
  "scale": {
    "users": 10000,
    "bank_accounts": 15000,
    "transactions": 50000
  },
  "messiness_config": {
    "date_format_variation": true,
    "numeric_format_inconsistency": true,
    "missing_field_probability": 0.05,
    "duplicate_probability": 0.02
  }
}
```

### AI Provider Keys (`.env`)
```bash
DEEPSEEK_API_KEY=sk-553a2062a03e4a88aec97575bd25d268
GEMINI_API_KEY=your_gemini_key_here
MAX_AI_PROMPT_TOKENS=1500
MAX_AI_RESPONSE_TOKENS=1500
```

---

## ğŸš§ Known Limitations & Future Enhancements

### Current Limitations
1. **Anomaly detection** is rule-based (1st-pass logic) â€” not ML-based
2. **GST/OCEN/MF/Insurance** analyzers have partial `calculation` metadata parity (transactions/credit/ONDC fully implemented)
3. **AI insights** subject to provider token limits (may truncate context for large datasets)

### Planned Enhancements
- ML-based anomaly detection (isolation forest, autoencoders)
- Richer calculation metadata across all analyzers
- Real AA integration (currently simulated)
- Time-series forecasting for cashflow prediction
- Interactive risk matrix charts

---

## ğŸ“š Documentation

- **[CUSTOMER_LENDING_FLOW.md](docs/CUSTOMER_LENDING_FLOW.md)** â€” detailed lending journey walkthrough
- **[data_dictionary.md](docs/data_dictionary.md)** â€” field-level data documentation
- **[FLOW.md](FLOW.md)** â€” pipeline execution flow diagram

---

## ğŸ¤ Contributing

This is a demo/prototype project. For production use:
1. Replace synthetic data generators with real AA connectors
2. Implement proper authentication & authorization
3. Add audit trails and compliance logging
4. Deploy backend/frontend with HTTPS
5. Set up database for persistent storage (currently file-based)

---

## ğŸ“„ License

MIT License â€” Free to use for educational and commercial purposes.

---

## ğŸ™ Acknowledgments

Built with adherence to:
- **RBI Account Aggregator Framework** (Master Directions)
- **Digital Personal Data Protection Act (DPDP) 2023**
- **GSTN API** specifications
- **ONDC Beckn Protocol** standards

---

**For questions or support**: Open an issue on GitHub or contact the maintainers.
